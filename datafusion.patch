diff --git a/Cargo.toml b/Cargo.toml
index ab3f427e..4dfaacc4 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -31,6 +31,49 @@ members = [
 ]
 exclude = ["datafusion-cli"]
 
+# Enable the version 2 feature resolver, which avoids unifying features for targets that are not being built
+#
+# Critically this prevents dev-dependencies from enabling features even when not building a target that
+# uses dev-dependencies, e.g. the library crate. This in turn ensures that we can catch invalid feature
+# flag combinations that would otherwise only surface in dependent crates
+#
+# Reference - https://doc.rust-lang.org/nightly/cargo/reference/features.html#feature-resolver-version-2
+#
+resolver = "2"
+
 [profile.release]
 codegen-units = 1
 lto = true
+
+[patch.crates-io]
+# https://github.com/samdenty/reqwest adds tokio features but names the package reqwest-wasm
+# https://github.com/milesforks/reqwest renames package to reqwest and bumps version
+reqwest = { version="0.11.16", git = "https://github.com/milesforks/reqwest"  }
+
+# ARROW: this git hash is ~2 commits before 26.0.0
+# arrow = { git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# arrow-array = { git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# arrow-buffer ={ git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# arrow-data = { git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# arrow-flight ={ git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# arrow-schema ={ git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# arrow-select ={ git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# parquet = { git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# parquet_derive ={ git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# parquet_derive_test = {git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+# object_store ={ git = "https://github.com/apache/arrow-rs", rev = "87ac05bcafd343d3d8ad3b519631d83090afeb1c" }
+
+# ARROW: uncomment these to develop from a local version of the arrow-rs repository
+arrow = { path = "../arrow-rs/arrow" }
+arrow-array = { path = "../arrow-rs/arrow-array" }
+arrow-buffer = { path = "../arrow-rs/arrow-buffer" }
+arrow-data = { path = "../arrow-rs/arrow-data" }
+arrow-flight = { path = "../arrow-rs/arrow-flight" }
+arrow-schema = { path = "../arrow-rs/arrow-schema" }
+arrow-select = { path = "../arrow-rs/arrow-select" }
+parquet = { path = "../arrow-rs/parquet" }
+parquet_derive = { path = "../arrow-rs/parquet_derive" }
+parquet_derive_test = { path = "../arrow-rs/parquet_derive_test" }
+object_store = { path = "../arrow-rs/object_store" }
+
+getrandom = { path = "../getrandom"}
diff --git a/README.md b/README.md
index 29113678..4ca01a91 100644
--- a/README.md
+++ b/README.md
@@ -17,6 +17,12 @@
   under the License.
 -->
 
+# Patch
+
+```
+export RUSTFLAGS="--cfg tokio_unstable" ; cargo build --verbose --target wasm32-wasi --package datafusion --package datafusion-expr --package datafusion-proto --package datafusion-common --no-default-features --features datafusion-proto/default,datafusion/regex_expressions,datafusion/unicode_expressions,datafusion/object_store,datafusion-common/apache-avro,datafusion-common/parquet
+```
+
 # DataFusion
 
 <img src="docs/source/_static/images/DataFusion-Logo-Background-White.svg" width="256" alt="logo"/>
diff --git a/datafusion/common/Cargo.toml b/datafusion/common/Cargo.toml
index d69bc197..74c23353 100644
--- a/datafusion/common/Cargo.toml
+++ b/datafusion/common/Cargo.toml
@@ -42,7 +42,17 @@ pyarrow = ["pyo3", "arrow/pyarrow"]
 apache-avro = { version = "0.14", default-features = false, features = ["snappy"], optional = true }
 arrow = { version = "25.0.0", default-features = false }
 cranelift-module = { version = "0.89.0", optional = true }
-object_store = { version = "0.5.0", default-features = false, optional = true }
+# object_store = { version = "0.5.0", default-features = false, optional = true }
+#"ring", "reqwest/json", "reqwest/stream", "chrono/serde", "quick-xml",
+# cloud = ["serde", "serde_json", "quick-xml", "reqwest", "reqwest/json", "reqwest/stream", "chrono/serde", "base64", "rand", "ring", "getrandom"]
+# ["serde", "serde_json", "reqwest", "base64", "rand"]#
+# tokio = { version = "1.0", features = ["rt", "macros", "io-util"] }
+object_store = {version = "0.5.1", default_features = false, features = ["serde", "serde_json", "quick-xml", "reqwest", "base64", "rand"], optional = true}
+reqwest = { version = "0.11.16", features = ["json", "stream", "blocking"] }
+
+# chrono = { version = "0.4.22", features = ["serde", "wasmbind","wasm-bindgen"], default_features = true }
+
+# object_store = "0.5.0"
 ordered-float = "3.0"
 parquet = { version = "25.0.0", default-features = false, optional = true }
 pyo3 = { version = "0.17.1", optional = true }
diff --git a/datafusion/core/Cargo.toml b/datafusion/core/Cargo.toml
index 7e9be303..ad8c779b 100644
--- a/datafusion/core/Cargo.toml
+++ b/datafusion/core/Cargo.toml
@@ -41,7 +41,7 @@ path = "src/lib.rs"
 # Used to enable the avro format
 avro = ["apache-avro", "num-traits", "datafusion-common/avro"]
 crypto_expressions = ["datafusion-physical-expr/crypto_expressions"]
-default = ["crypto_expressions", "regex_expressions", "unicode_expressions"]
+default = ["regex_expressions", "unicode_expressions"] # -"crypto_expressions",
 # Used for testing ONLY: causes all values to hash to the same value (test for collisions)
 force_hash_collisions = []
 # Used to enable JIT code generation
@@ -54,15 +54,17 @@ simd = ["arrow/simd"]
 unicode_expressions = ["datafusion-physical-expr/regex_expressions", "datafusion-sql/unicode_expressions"]
 
 [dependencies]
-ahash = { version = "0.8", default-features = false, features = ["runtime-rng"] }
+ahash = { version = "^0.8", default-features = false, features = ["runtime-rng"] }
 apache-avro = { version = "0.14", optional = true }
 arrow = { version = "25.0.0", features = ["prettyprint"] }
-async-compression = { version = "0.3.14", features = ["bzip2", "gzip", "futures-io", "tokio"] }
+# https://github.com/Nemo157/async-compression/issues/142
+async-compression = { version = "0.3.15", features = ["gzip", "tokio", "futures-io"] } # -"bzip2", -"tokio"
 async-trait = "0.1.41"
 bytes = "1.1"
-bzip2 = "0.4.3"
+bzip2 = { version = "0.4.3", optional = true }
+# chrono = { git = "https://github.com/chronotope/chrono", version = "0.5.0-alpha.1", default-features = false, features = ["serde", "wasmbind", "wasm-bindgen"] }
 chrono = { version = "0.4.22", default-features = false }
-datafusion-common = { path = "../common", version = "13.0.0", features = ["parquet", "object_store"] }
+datafusion-common = { path = "../common", version = "13.0.0", default-features = false, features = [ "parquet", "object_store"] } #-"object_store", -py03, "apache-avro", "cranelift-module",
 datafusion-expr = { path = "../expr", version = "13.0.0" }
 datafusion-jit = { path = "../jit", version = "13.0.0", optional = true }
 datafusion-optimizer = { path = "../optimizer", version = "13.0.0" }
@@ -73,15 +75,20 @@ flate2 = "1.0.24"
 futures = "0.3"
 glob = "0.3.0"
 hashbrown = { version = "0.12", features = ["raw"] }
+instant = { version = "0.1" }
 itertools = "0.10"
 lazy_static = { version = "^1.4.0" }
 log = "^0.4"
 num-traits = { version = "0.2", optional = true }
 num_cpus = "1.13.0"
-object_store = "0.5.0"
+# https://github.com/apache/arrow-rs/blob/master/object_store/Cargo.toml
+# object_store = {version = "0.5.1", default_features = false, features = ["cloud"], optional = false}
+object_store = {version = "0.5.1", default_features = false, features = ["serde", "serde_json", "reqwest", "base64", "rand"], optional = true} #"ring", "reqwest/json", "reqwest/stream", "chrono/serde", "quick-xml",
 ordered-float = "3.0"
 parking_lot = "0.12"
-parquet = { version = "25.0.0", features = ["arrow", "async"] }
+# note: parquet is using version of zstd which does not compile to wasm
+# zstd = { version = "0.11.1", optional = true, default-features = false }
+parquet = { version = "25.0.0", default_features = false,  features = ["arrow", "async", "snap", "brotli", "flate2", "base64"] } # -lz4, -zstd
 paste = "^1.0"
 percent-encoding = "2.2.0"
 pin-project-lite = "^0.2.7"
@@ -91,11 +98,15 @@ rayon = { version = "1.5", optional = true }
 smallvec = { version = "1.6", features = ["union"] }
 sqlparser = "0.25"
 tempfile = "3"
-tokio = { version = "1.0", features = ["macros", "rt", "rt-multi-thread", "sync", "fs", "parking_lot"] }
-tokio-stream = "0.1"
-tokio-util = { version = "0.7.4", features = ["io"] }
+# tokio = { version = "1.0", features = ["macros", "rt", "rt-multi-thread", "sync", "fs", "parking_lot"] } # -"rt-multi-thread", -"fs", -"parking_lot"
+# TODO: rustls support ? https://github.com/samdenty/reqwest/blob/master/Cargo.toml
+tokio = { version = "1.0", features = ["rt", "macros", "io-util", "parking_lot"] }
+tokio-stream = { version = "0.1", default_features = false, features = [] } #-"io_util"
+tokio-util = { version = "0.7.4", features = ["io"] }  #-"io"
 url = "2.2"
 uuid = { version = "1.0", features = ["v4"] }
+# force reqwest-wasm to resolve (via patch in our downstream consumer) so tokio::spawn_blocking can work
+reqwest = { version = "0.11.16", features = ["json", "stream", "blocking"] }
 
 [dev-dependencies]
 arrow = { version = "25.0.0", features = ["prettyprint", "dyn_cmp_dict"] }
diff --git a/datafusion/core/benches/parquet_query_sql.rs b/datafusion/core/benches/parquet_query_sql.rs
index e9204f86..b2fdd141 100644
--- a/datafusion/core/benches/parquet_query_sql.rs
+++ b/datafusion/core/benches/parquet_query_sql.rs
@@ -27,6 +27,7 @@ use criterion::{criterion_group, criterion_main, Criterion};
 use datafusion::prelude::{SessionConfig, SessionContext};
 use datafusion::scheduler::Scheduler;
 use futures::stream::StreamExt;
+use instant::Instant;
 use parquet::arrow::ArrowWriter;
 use parquet::file::properties::{WriterProperties, WriterVersion};
 use rand::distributions::uniform::SampleUniform;
@@ -37,7 +38,6 @@ use std::io::Read;
 use std::ops::Range;
 use std::path::Path;
 use std::sync::Arc;
-use std::time::Instant;
 use tempfile::NamedTempFile;
 
 /// The number of batches to write
diff --git a/datafusion/core/src/datasource/file_format/file_type.rs b/datafusion/core/src/datasource/file_format/file_type.rs
index f08a21ca..901fc851 100644
--- a/datafusion/core/src/datasource/file_format/file_type.rs
+++ b/datafusion/core/src/datasource/file_format/file_type.rs
@@ -20,9 +20,10 @@
 use crate::error::{DataFusionError, Result};
 use std::io::Error;
 
-use async_compression::tokio::bufread::{
-    BzDecoder as AsyncBzDecoder, GzipDecoder as AsyncGzDecoder,
-};
+#[cfg(feature = "dep:bzip2")]
+use async_compression::tokio::bufread::BzDecoder as AsyncBzDecoder;
+use async_compression::tokio::bufread::GzipDecoder as AsyncGzDecoder;
+#[cfg(feature = "dep:bzip2")]
 use bzip2::read::BzDecoder;
 
 use crate::datasource::file_format::avro::DEFAULT_AVRO_EXTENSION;
@@ -46,6 +47,7 @@ pub trait GetExt {
 pub enum FileCompressionType {
     /// Gzip-ed file
     GZIP,
+    #[cfg(feature = "dep:bzip2")]
     /// Bzip2-ed file
     BZIP2,
     /// Uncompressed file
@@ -56,6 +58,7 @@ impl GetExt for FileCompressionType {
     fn get_ext(&self) -> String {
         match self {
             FileCompressionType::GZIP => ".gz".to_owned(),
+            #[cfg(feature = "dep:bzip2")]
             FileCompressionType::BZIP2 => ".bz2".to_owned(),
             FileCompressionType::UNCOMPRESSED => "".to_owned(),
         }
@@ -69,6 +72,7 @@ impl FromStr for FileCompressionType {
         let s = s.to_uppercase();
         match s.as_str() {
             "GZIP" | "GZ" => Ok(FileCompressionType::GZIP),
+            #[cfg(feature = "dep:bzip2")]
             "BZIP2" | "BZ2" => Ok(FileCompressionType::BZIP2),
             "" => Ok(FileCompressionType::UNCOMPRESSED),
             _ => Err(DataFusionError::NotImplemented(format!(
@@ -104,6 +108,7 @@ impl FileCompressionType {
                 ReaderStream::new(AsyncGzDecoder::new(StreamReader::new(s)))
                     .map_err(err_converter),
             ),
+            #[cfg(feature = "dep:bzip2")]
             FileCompressionType::BZIP2 => Box::new(
                 ReaderStream::new(AsyncBzDecoder::new(StreamReader::new(s)))
                     .map_err(err_converter),
@@ -119,6 +124,7 @@ impl FileCompressionType {
     ) -> Box<dyn std::io::Read + Send> {
         match self {
             FileCompressionType::GZIP => Box::new(GzDecoder::new(r)),
+            #[cfg(feature = "dep:bzip2")]
             FileCompressionType::BZIP2 => Box::new(BzDecoder::new(r)),
             FileCompressionType::UNCOMPRESSED => Box::new(r),
         }
diff --git a/datafusion/core/src/datasource/listing/table.rs b/datafusion/core/src/datasource/listing/table.rs
index deaa0924..5117000f 100644
--- a/datafusion/core/src/datasource/listing/table.rs
+++ b/datafusion/core/src/datasource/listing/table.rs
@@ -17,7 +17,10 @@
 
 //! The table implementation.
 
-use ahash::HashMap;
+// TODO: I just naively replaced ahash::HashMap with std::collections::HashMap
+//       (and I'm not sure it will actually work or won't break anything)
+// use ahash::HashMap;
+use std::collections::HashMap;
 use std::str::FromStr;
 use std::{any::Any, sync::Arc};
 
diff --git a/datafusion/core/src/datasource/listing/url.rs b/datafusion/core/src/datasource/listing/url.rs
index d1a527f2..c13761c6 100644
--- a/datafusion/core/src/datasource/listing/url.rs
+++ b/datafusion/core/src/datasource/listing/url.rs
@@ -100,8 +100,10 @@ impl ListingTableUrl {
 
         let path = std::path::Path::new(prefix).canonicalize()?;
         let url = match path.is_file() {
-            true => Url::from_file_path(path).unwrap(),
-            false => Url::from_directory_path(path).unwrap(),
+            // true => Url::from_file_path(path).unwrap(),
+            // false => Url::from_directory_path(path).unwrap(),
+            true => Url::parse("blob:https://example.com/foo").unwrap(),
+            false => Url::parse("blob:https://example.com/bar").unwrap(),
         };
 
         Ok(Self::new(url, glob))
diff --git a/datafusion/core/src/datasource/object_store.rs b/datafusion/core/src/datasource/object_store.rs
index 69b18313..f7c51e81 100644
--- a/datafusion/core/src/datasource/object_store.rs
+++ b/datafusion/core/src/datasource/object_store.rs
@@ -20,7 +20,7 @@
 //! and query data inside these systems.
 
 use datafusion_common::{DataFusionError, Result};
-use object_store::local::LocalFileSystem;
+use object_store::memory::InMemory;
 use object_store::ObjectStore;
 use parking_lot::RwLock;
 use std::collections::HashMap;
@@ -162,7 +162,7 @@ impl ObjectStoreRegistry {
     /// created lazily, on-demand by the provided [`ObjectStoreProvider`]
     pub fn new_with_provider(provider: Option<Arc<dyn ObjectStoreProvider>>) -> Self {
         let mut map: HashMap<String, Arc<dyn ObjectStore>> = HashMap::new();
-        map.insert("file://".to_string(), Arc::new(LocalFileSystem::new()));
+        map.insert("file://".to_string(), Arc::new(InMemory::new()));
         Self {
             object_stores: RwLock::new(map),
             provider,
@@ -267,7 +267,6 @@ mod tests {
         let url = ListingTableUrl::parse("hdfs://localhost:8020/key").unwrap();
         sut.get_by_url(&url).unwrap();
     }
-
     #[test]
     fn test_get_by_url_s3() {
         let sut = ObjectStoreRegistry::default();
@@ -275,7 +274,6 @@ mod tests {
         let url = ListingTableUrl::parse("s3://bucket/key").unwrap();
         sut.get_by_url(&url).unwrap();
     }
-
     #[test]
     fn test_get_by_url_file() {
         let sut = ObjectStoreRegistry::default();
diff --git a/datafusion/core/src/execution/memory_manager.rs b/datafusion/core/src/execution/memory_manager.rs
index 48d4ca3c..369ebd4d 100644
--- a/datafusion/core/src/execution/memory_manager.rs
+++ b/datafusion/core/src/execution/memory_manager.rs
@@ -20,13 +20,13 @@
 use crate::error::{DataFusionError, Result};
 use async_trait::async_trait;
 use hashbrown::HashSet;
+use instant::{Duration, Instant};
 use log::{debug, warn};
 use parking_lot::{Condvar, Mutex};
 use std::fmt;
 use std::fmt::{Debug, Display, Formatter};
 use std::sync::atomic::{AtomicUsize, Ordering};
 use std::sync::Arc;
-use std::time::{Duration, Instant};
 
 static CONSUMER_ID: AtomicUsize = AtomicUsize::new(0);
 
diff --git a/datafusion/core/src/physical_plan/analyze.rs b/datafusion/core/src/physical_plan/analyze.rs
index 8134ee7d..29c70ce5 100644
--- a/datafusion/core/src/physical_plan/analyze.rs
+++ b/datafusion/core/src/physical_plan/analyze.rs
@@ -18,7 +18,8 @@
 //! Defines the ANALYZE operator
 
 use std::sync::Arc;
-use std::{any::Any, time::Instant};
+use std::any::Any;
+use instant::Instant;
 
 use crate::{
     error::{DataFusionError, Result},
diff --git a/datafusion/core/src/physical_plan/file_format/file_stream.rs b/datafusion/core/src/physical_plan/file_format/file_stream.rs
index df12f310..f352c5db 100644
--- a/datafusion/core/src/physical_plan/file_format/file_stream.rs
+++ b/datafusion/core/src/physical_plan/file_format/file_stream.rs
@@ -21,11 +21,11 @@
 //! Note: Most traits here need to be marked `Sync + Send` to be
 //! compliant with the `SendableRecordBatchStream` trait.
 
+use instant::Instant;
 use std::collections::VecDeque;
 use std::pin::Pin;
 use std::sync::Arc;
 use std::task::{Context, Poll};
-use std::time::Instant;
 
 use arrow::datatypes::SchemaRef;
 use arrow::{error::Result as ArrowResult, record_batch::RecordBatch};
diff --git a/datafusion/core/src/physical_plan/joins/cross_join.rs b/datafusion/core/src/physical_plan/joins/cross_join.rs
index 7a35116a..93a4a1bc 100644
--- a/datafusion/core/src/physical_plan/joins/cross_join.rs
+++ b/datafusion/core/src/physical_plan/joins/cross_join.rs
@@ -35,8 +35,8 @@ use crate::physical_plan::{
 use crate::{error::Result, scalar::ScalarValue};
 use async_trait::async_trait;
 use datafusion_physical_expr::PhysicalSortExpr;
+use instant::Instant;
 use log::debug;
-use std::time::Instant;
 
 use super::utils::{check_join_is_valid, OnceAsync, OnceFut};
 
diff --git a/datafusion/core/src/physical_plan/joins/hash_join.rs b/datafusion/core/src/physical_plan/joins/hash_join.rs
index ff036b78..21b9fc4f 100644
--- a/datafusion/core/src/physical_plan/joins/hash_join.rs
+++ b/datafusion/core/src/physical_plan/joins/hash_join.rs
@@ -34,10 +34,11 @@ use arrow::{
         UInt8Type,
     },
 };
+use instant::Instant;
 use smallvec::{smallvec, SmallVec};
 use std::sync::Arc;
+use std::vec;
 use std::{any::Any, usize};
-use std::{time::Instant, vec};
 
 use futures::{ready, Stream, StreamExt, TryStreamExt};
 
diff --git a/datafusion/core/src/physical_plan/metrics/value.rs b/datafusion/core/src/physical_plan/metrics/value.rs
index 5c3aeb4d..9d690613 100644
--- a/datafusion/core/src/physical_plan/metrics/value.rs
+++ b/datafusion/core/src/physical_plan/metrics/value.rs
@@ -24,9 +24,10 @@ use std::{
         atomic::{AtomicUsize, Ordering},
         Arc,
     },
-    time::{Duration, Instant},
 };
 
+use instant::{Duration, Instant};
+
 use parking_lot::Mutex;
 
 use chrono::{DateTime, Utc};
diff --git a/datafusion/core/src/physical_plan/sorts/sort.rs b/datafusion/core/src/physical_plan/sorts/sort.rs
index 763c7c55..0999fca3 100644
--- a/datafusion/core/src/physical_plan/sorts/sort.rs
+++ b/datafusion/core/src/physical_plan/sorts/sort.rs
@@ -598,7 +598,8 @@ async fn spill_partial_sorted_stream(
 ) -> Result<()> {
     let (sender, receiver) = tokio::sync::mpsc::channel(2);
     let path: PathBuf = path.into();
-    let handle = task::spawn_blocking(move || write_sorted(receiver, path, schema));
+    // FIXME: this should be spawn_blocking but that wasn't compiling
+    let handle = task::spawn(async move { write_sorted(receiver, path, schema) });
     while let Some(item) = in_mem_stream.next().await {
         sender.send(item).await.ok();
     }
@@ -620,11 +621,13 @@ fn read_spill_as_stream(
         Sender<ArrowResult<RecordBatch>>,
         Receiver<ArrowResult<RecordBatch>>,
     ) = tokio::sync::mpsc::channel(2);
-    let join_handle = task::spawn_blocking(move || {
+    // FIXME: this should be spawn_blocking but that wasn't compiling
+    let join_handle = task::spawn(async move {
         if let Err(e) = read_spill(sender, path.path()) {
             error!("Failure while reading spill file: {:?}. Error: {}", path, e);
         }
     });
+
     Ok(RecordBatchReceiverStream::create(
         &schema,
         receiver,
diff --git a/datafusion/core/src/test/mod.rs b/datafusion/core/src/test/mod.rs
index bce27767..60738f85 100644
--- a/datafusion/core/src/test/mod.rs
+++ b/datafusion/core/src/test/mod.rs
@@ -33,7 +33,9 @@ use array::ArrayRef;
 use arrow::array::{self, Array, Decimal128Builder, Int32Array};
 use arrow::datatypes::{DataType, Field, Schema, SchemaRef};
 use arrow::record_batch::RecordBatch;
+#[cfg(feature = "dep:bzip2")]
 use bzip2::write::BzEncoder;
+#[cfg(feature = "dep:bzip2")]
 use bzip2::Compression as BzCompression;
 use flate2::write::GzEncoder;
 use flate2::Compression as GzCompression;
@@ -115,6 +117,7 @@ pub fn partitioned_file_groups(
             FileCompressionType::GZIP => {
                 Box::new(GzEncoder::new(file, GzCompression::default()))
             }
+            #[cfg(feature = "dep:bzip2")]
             FileCompressionType::BZIP2 => {
                 Box::new(BzEncoder::new(file, BzCompression::default()))
             }
diff --git a/datafusion/optimizer/Cargo.toml b/datafusion/optimizer/Cargo.toml
index 729e2306..dc3fd54b 100644
--- a/datafusion/optimizer/Cargo.toml
+++ b/datafusion/optimizer/Cargo.toml
@@ -39,7 +39,9 @@ unicode_expressions = []
 [dependencies]
 arrow = { version = "25.0.0", features = ["prettyprint"] }
 async-trait = "0.1.41"
+# chrono = { git = "https://github.com/chronotope/chrono", version = "0.5.0-alpha.1", default-features = false }
 chrono = { version = "0.4.22", default-features = false }
+instant = { version = "0.1" }
 datafusion-common = { path = "../common", version = "13.0.0" }
 datafusion-expr = { path = "../expr", version = "13.0.0" }
 datafusion-physical-expr = { path = "../physical-expr", version = "13.0.0" }
diff --git a/datafusion/optimizer/src/optimizer.rs b/datafusion/optimizer/src/optimizer.rs
index 7c37284e..4a3664cc 100644
--- a/datafusion/optimizer/src/optimizer.rs
+++ b/datafusion/optimizer/src/optimizer.rs
@@ -39,9 +39,9 @@ use crate::unwrap_cast_in_comparison::UnwrapCastInComparison;
 use chrono::{DateTime, Utc};
 use datafusion_common::{DataFusionError, Result};
 use datafusion_expr::logical_plan::LogicalPlan;
+use instant::Instant;
 use log::{debug, trace, warn};
 use std::sync::Arc;
-use std::time::Instant;
 
 /// `OptimizerRule` transforms one ['LogicalPlan'] into another which
 /// computes the same results, but in a potentially more efficient
diff --git a/datafusion/physical-expr/Cargo.toml b/datafusion/physical-expr/Cargo.toml
index e0703829..832ef2e8 100644
--- a/datafusion/physical-expr/Cargo.toml
+++ b/datafusion/physical-expr/Cargo.toml
@@ -43,7 +43,8 @@ ahash = { version = "0.8", default-features = false, features = ["runtime-rng"]
 arrow = { version = "25.0.0", features = ["prettyprint"] }
 blake2 = { version = "^0.10.2", optional = true }
 blake3 = { version = "1.0", optional = true }
-chrono = { version = "0.4.22", default-features = false }
+# chrono = { git = "https://github.com/chronotope/chrono", version = "0.5.0-alpha.1", default-features = true, features = ["wasmbind", "wasm-bindgen"] }
+chrono = { version = "0.4.22" }
 datafusion-common = { path = "../common", version = "13.0.0" }
 datafusion-expr = { path = "../expr", version = "13.0.0" }
 datafusion-row = { path = "../row", version = "13.0.0" }
diff --git a/datafusion/physical-expr/src/datetime_expressions.rs b/datafusion/physical-expr/src/datetime_expressions.rs
index 0214d1bf..0b37a5cc 100644
--- a/datafusion/physical-expr/src/datetime_expressions.rs
+++ b/datafusion/physical-expr/src/datetime_expressions.rs
@@ -189,7 +189,8 @@ fn quarter_month(date: &NaiveDateTime) -> u32 {
 }
 
 fn date_trunc_single(granularity: &str, value: i64) -> Result<i64> {
-    let value = timestamp_ns_to_datetime(value).with_nanosecond(0);
+    // timestamp_ns_to_datetime now returns Option (https://github.com/apache/arrow-rs/pull/2894/files)
+    let value = timestamp_ns_to_datetime(value).unwrap().with_nanosecond(0);
     let value = match granularity {
         "second" => value,
         "minute" => value.and_then(|d| d.with_second(0)),
